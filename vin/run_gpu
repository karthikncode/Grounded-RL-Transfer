#!/bin/bash

if [ -z "$1" ]
  then echo "Please provide the port, e.g.  ./run_cpu 6000 "; exit 0
fi
DEBUG=false
ENV="gvgai"
PORT=$1
FRAMEWORK="alewrap"
exp_folder=$2
num_actions=5  # This has been overridden in the code - auto inferred. 
game_num=${3:-"0"}
level_num=${4:-"0"}
test_game_num=${5:-"0"}
test_level_num=${6:-"1"}
vin_k=${7:-5}
lr=${8:-0.0005}
minibatch_size=${9:-32}
expert_net=${10:-"nil"}
eps_begin=${11:-0.1}  # Beginning value of epsilon
netfile=${12:-"\"vin_gvgai\""}
text_fraction=${13:-1}
mode=${14:-"vin"}
textrep=${15:-"lstm"}

vin_only=false  # Set this to true for only the VIN component.
if [ "$mode" = "textdqn" ] ; then  # text-DQN
    reactive=true
    simple=false
elif [ "$mode" = "dqn" ] ; then  # dqn
    simple=true
    reactive=true
elif [ "$mode" = "vin" ] ; then  # dqn
    simple=true
    reactive=false
elif [ "$mode" = "textvin" ] ; then
    reactive=false  # Set this to true if you want text DQN.
    simple=false
else  
    echo "Unknown mode! Need a value in [dqn, textdqn, vin, textvin]"
    exit;
fi;

if [ "$textrep" = "lstm" ]; then
    lstm=true # Set this to true if you want LSTM to handle the text
else
    lstm=false
fi;

object_filter=true  # Set this to true to use object embeddings.

text_folder="\"text\""
if [ "$DEBUG" = true ] ; then
    epoch=1000;
    learn_start=$((2 * epoch  - 10))
    eval_freq=$((1 * epoch))
else
    # This are the values used in actual training. 
    epoch=1000;
    learn_start=$((5 * epoch))
    eval_freq=$((5 * epoch))
fi;

lr_end=0.00005
game_path=$PWD"/roms/"
env_params="useRGB=true"
agent="NeuralQLearner"
n_replay=1
update_freq=4
actrep=4
discount=0.8
seed=$PORT
pool_frms_type="\"max\""
pool_frms_size=2
initial_priority="false"
replay_memory=250000
eps_end=0.1
eps_endt=$replay_memory
agent_type="gvgai"
preproc_net="\"net_downsample_2x_full_y\""
agent_name=$exp_folder"/"$agent_type"_"$1
state_dim=288 #330  # This has been overridden in the code - auto inferred. 

steps=5000000
eval_steps=10000
prog_freq=$((5*epoch))
save_freq=$((10*epoch))
eval_episodes=10
hist_len=1

ncols=1

agent_params="text_folder="$text_folder",lstm="$lstm",simple="$simple",reactive="$reactive",vin_only="$vin_only",object_filter="$object_filter",num_actions="$num_actions",vin_k="$vin_k",lr="$lr",ep="$eps_begin",ep_end="$eps_end",ep_endt="$eps_endt",discount="$discount",hist_len="$hist_len",learn_start="$learn_start",replay_memory="$replay_memory",update_freq="$update_freq",n_replay="$n_replay",network="$netfile",expert_network="$expert_net",preproc="$preproc_net",state_dim="$state_dim",minibatch_size="$minibatch_size",rescale_r=1,ncols="$ncols",bufferSize=512,valid_size=100,target_q="$((5*epoch))",clip_delta=1,min_reward=-1,max_reward=1"
gpu=0
random_starts=30
pool_frms="type="$pool_frms_type",size="$pool_frms_size
num_threads=1

args="-zmq_port $PORT -text_fraction $text_fraction -game_num $game_num -test_game_num $test_game_num -level_num $level_num -test_level_num $test_level_num -exp_folder $exp_folder -num_actions $num_actions -framework $FRAMEWORK -game_path $game_path -name $agent_name -env $ENV -env_params $env_params -agent $agent -agent_params $agent_params -steps $steps -eval_freq $eval_freq -eval_steps $eval_steps -eval_episodes $eval_episodes -prog_freq $prog_freq -save_freq $save_freq -actrep $actrep -gpu $gpu -random_starts $random_starts -pool_frms $pool_frms -seed $seed -threads $num_threads"
echo $args

cd dqn
mkdir -p $exp_folder;
OMP_NUM_THREADS=1 th train_agent.lua $args
